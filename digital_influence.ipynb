{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Collection and Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook:\n",
    "\n",
    "1. Attaching to the Twitter API\n",
    "2. Searching for a specific user\n",
    "3. Searching for a specific topic\n",
    "4. Extending the search and working with multi-level JSON Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attaching to the Twitter API\n",
    "\n",
    "## Questions & Objectives\n",
    "\n",
    "* Setting up access and validity signing\n",
    "* Setting up a handler to manage the connection\n",
    "* Running a test search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we will download the libraries that deal with accessing the API (tweepy) and working with the JSON data (json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell now to import the libraries\n",
    "\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up the variables that hold the validation keys. You need to add your keys (tokens) and secrets in the spaces below. Make sure to put them between the speech marks and make sure there is not extra spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in your keys and secrets then run this cell\n",
    "\n",
    "access_key = ''\n",
    "access_secret = ''\n",
    "api_key = ''\n",
    "api_secret = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up the authication handler. We pass the keys and secrets as below and then set up the api object. We can then use this object to attach to the API each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the connection we will run a test query.\n",
    "\n",
    "We use the API object and we are going to ask for some of the tweets from users that you follow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for a Specific User\n",
    "\n",
    "* Search for a specific user\n",
    "* Retrieve data from the Twitter API\n",
    "* Call specific items from the JSON data object\n",
    "* Look at the full JSON data\n",
    "\n",
    "We will now look for tweets from a specific person. To do this we need their Twitter name. If you go to https://twitter.com/BarackObama you can see the twitter name under the main name. You can see it has a @ sign in front that we remove.  \n",
    "\n",
    "For this we use the **get_user** method from the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create an object and call the information on the user Barack Obama and hold it in the object.\n",
    "user = api.get_user('BarackObama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This object is in JSON tuples.\n",
    "# We can call the tuples and print their content. \n",
    "# we will look more at JSON later\n",
    "# We can print the screen name as below\n",
    "\n",
    "print(user.screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print the number of followers -- check this is correct on the link to the Twitter page\n",
    "\n",
    "print(user.followers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Can print the user description \n",
    "\n",
    "print(user.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all of the user information in it's raw format we can type:\n",
    "\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minitask\n",
    "\n",
    "* Try using the information from the user print out to access the other information.\n",
    "* See if you can work out how to get to the nested tuples\n",
    "* Try and look at another user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get tweets from the API user timeline\n",
    "# This time we call the user_timeline method again with the BarackObama user method\n",
    "# Here we call the last two tweets\n",
    "# These are retured in a list object\n",
    "\n",
    "new_tweets = api.user_timeline(screen_name = 'BarackObama',count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can tweet the first tweet (which remember is 0 in a list)\n",
    "new_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for a Topic\n",
    "\n",
    "* Search the twitter API using a key word\n",
    "* Retrieve the text from a single tweets\n",
    "* Retrieve the text from multiple tweets\n",
    "* Process and clean the text\n",
    "* Visualise the text\n",
    "\n",
    "We will now look for tweets that contain a specific word. \n",
    "\n",
    "For this we use the **search** method from the Twitter API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are looking for the word covid\n",
    "# We are asking for 10 english tweets to be returned\n",
    "# This is returned as a list\n",
    "\n",
    "brexit_tweets = api.search(q='covid', lang='en', count='10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can print out the first in the list \n",
    "\n",
    "brexit_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we can't just call the JSON from the object (like we did with the user object)\n",
    "We have to deal with the JSON directly. We do this using the json function\n",
    "Then we can call all of the tuples as a dictionary object. \n",
    "\n",
    "(remember a tuple take the form ['text':'this is tweet text'] this means that we can call for the content of the tuple by the key of the tuple.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can see all of the json in a nice format\n",
    "\n",
    "brexit_tweets[0]._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can just call the text\n",
    "\n",
    "brexit_tweets[0]._json['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can text put the text into it's own list and just work with just the text\n",
    "\n",
    "tweets_text = []\n",
    "for each in brexit_tweets:\n",
    "    tweets_text.append(each._json['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see how we have put this into a list\n",
    "\n",
    "print(tweets_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can treat this like we did in earlier badges\n",
    "# For example we can turn it into a string and tokenise it\n",
    "\n",
    "tweets_string = \" \".join(tweets_text)\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(tweets_string)\n",
    "print(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can clean it up like we did earlier, making it all lowercase and removing stopwords\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "lowercase_tokens = [token.lower() for token in tokens]\n",
    "remove_these = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))\n",
    "filtered_text = [word \n",
    "                 for word in lowercase_tokens \n",
    "                 if not word in remove_these]\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can produce word frequencies\n",
    "from collections import Counter\n",
    "simple_frequencies_dict = Counter(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And word clouds\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "cloud = WordCloud(max_font_size=80,colormap=\"hsv\").generate_from_frequencies(simple_frequencies_dict)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minitask\n",
    "\n",
    "* Try using a visualisation method or a search method you have used before to visualize the text\n",
    "* Try searching for a differnt word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the search and working with multi-level JSON Data\n",
    "\n",
    "* Search the Twitter API using an extended query with multiple terms\n",
    "* Search using a tweepy cursor to retrieve more data\n",
    "* Look at nested data from the JSON\n",
    "\n",
    "We will now look for tweets that contain several words. We can combine query words with the operator 'OR'. This operator say give me tweets that contain word1 or word2. You might want to do this with related words on the same topic or use it to cover multiple spellings or typos. \n",
    "\n",
    "For this we use the **search** method from the Twitter API.\n",
    "\n",
    "We want to gather more data than we did before. The search method limits the data we can retrieve. To extend the amount of data we retrieve we use a Tweepy Cursor. Twitter returns multiple pages of data. Almost like a book, but it will only give you one page at a time. Before we only took the first page. This time we will page through the extended version using a cursor object. The cursor maintain the connection with the API and allows us to ask for the next page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set up a list to hold the tweets so we can then append to it as we iterate through \n",
    "# Previously we created it in the search but here we need it created so we can add to it\n",
    "\n",
    "covid_tweets = []\n",
    "\n",
    "# We then set up a tweepy cursor to maintain the connection\n",
    "# We set up the query with the OR operator\n",
    "# We iterate through the pages from the API using a for loop\n",
    "# We append the content to a list\n",
    "for page in tweepy.Cursor(api.search, q='covid OR covid19 OR COVID OR COVID19 or #covid', lang='en', min_retweets=\"1000\").pages(100):\n",
    "    covid_tweets.append(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the text from the first tweet\n",
    "\n",
    "print(covid_tweets[0]._json['text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter data is nested.\n",
    "\n",
    "This means that it can contain items within items. \n",
    "\n",
    "For example hashtags, user mentions, and URL's are contained within an entity tuple.\n",
    "\n",
    "This looks like:\n",
    "\n",
    "['entities': ['hashtags': ['hashtag1', 'hashtag2'], ['user_mentions': 'barackobama'], ['url':'www.bbc.co.uk']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hashtags are contained in a list within the entity tuple\n",
    "# This means we need to call the entity, hashtag tuple and then iterate through the list\n",
    "# We set up a list to hold the hashtags so we can then append to it as we iterate through\n",
    "# We iterate through each tweet, and then through the hashtags in the list\n",
    "# We add them to the list\n",
    "\n",
    "covid_hashtags = []\n",
    "for each in covid_tweets:\n",
    "   for hashtag in each._json['entities']['hashtags']:\n",
    "    covid_hashtags.append(hashtag['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can them visualise these hashtags in the ways we have learnt before\n",
    "\n",
    "hashtag_string = \" \".join(covid_hashtags)\n",
    "tokens = word_tokenize(hashtag_string)\n",
    "simple_frequencies_dict_covid = Counter(tokens)\n",
    "cloud = WordCloud(max_font_size=80, colormap=\"viridis\", background_color='white',).generate_from_frequencies(simple_frequencies_dict_covid)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minitask\n",
    "\n",
    "* Try using the creating a visualisation with a different nested item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have this as a task -- look for another item of interest maybe alter to be URL's?\n",
    "#covid_mentions = []\n",
    "#for each in covid_tweets:\n",
    "#   for mention in each._json['entities']['user_mentions']:\n",
    "#        covid_mentions.append(mention['name'])\n",
    "#people_dict=Counter(covid_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cloud = WordCloud(max_font_size=80,background_color='white',colormap=\"viridis\").generate_from_frequencies(people_dict)\n",
    "#plt.figure(figsize=(16,12))\n",
    "#plt.imshow(cloud, interpolation='bilinear')\n",
    "#plt.axis('off')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid_tweets = []\n",
    "#for page in tweepy.Cursor(api.search, q='brexit', lang='en', min_retweets=\"1000\").pages(100):\n",
    "#    covid_tweets.append(page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
